---
title: "Principal component analysis"
bibliography: /home/pmassicotte/Documents/library.bib
output:
  revealjs::revealjs_presentation:
    incremental: true
    theme: moon
    transition: convex
    highlight: kate
    center: true
    self_contained: true
    fig_width: 5
    fig_height: 4
    fig_caption: false
    css: styles.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  fig.width = 7,
  fig.height = 5,
  fig.align = "center"
)

library(ggplot2)
library(dplyr)
library(vegan)
library(extrafont)
loadfonts(quiet = TRUE)

theme_set(theme_bw(base_size = 16, base_family = "Open Sans"))
```

## Canonical analysis
<!-- https://www.youtube.com/watch?v=I5GxNzKLIoU -->


- Family of statistical analyzes that allows to **study and explore** a data set of quantitative variables.


- Aimed at **reducing the dimensionality** of a data set while retaining much of the variation present in the data.
    - Create new axes that explain most of the data variation.
    - Project data on these new axes.


- Visualization techniques are limited to 1D, 2D or 3D data.
    - By reducing the dimensionality of data, canonical analysis make possible to plot in 2D.

## Type of canonical analysis

1. Canonical Correlation Analysis (CCA)

2. **Principal Component Analysis (PCA)**

3. Linear discriminant analysis (LDA)

4. **Redundancy Analysis (RDA)**

## Principal component analysis (PCA)

- One of the goals behind PCA is to graphically represent the essential
information contained in a (quantitative) data table.

- Useful way to discover (hidden) patterns in the data by **compressing** data.

- Not performed directly on the data but on either the **covariance** or **correlation** matrix of the data.

## Matrix structure

PCA analysis is applied to rectangular data format.

$$
X_{n,p} =
 \begin{bmatrix}
  a_{1,1} & a_{1,2} & \cdots & a_{1,p} \\
  a_{2,1} & a_{2,2} & \cdots & a_{2,p} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{n,1} & a_{n,2} & \cdots & a_{m,p}
 \end{bmatrix}
$$

<br>

$n$ objects in the rows (**observations**)

$p$ quantitative variables in the columns (**variables**)

## Example of high dimensional data

#### mtcars dataset

<small>
```{r, echo = FALSE, results="asis"}
print(xtable::xtable(head(mtcars, 10)[, 1:7]), type = "html", NA.string = "NA")
```
</small>

## Visualisation

One option to visualize this dataset is to look at all pairs of correlation.

```{r, fig.width = 5, fig.height = 4}
languageR::pairscor.fnc(mtcars[, 1:7])
```

## A simple example 

The main idea of a PCA is to find out vector (also called **loadings** or **eigenvectors**) that explain most of the variation in the data.

For this example we will perform a PCA on two variables from the `mtcars` data set.

```{r}
# Extract two variables from mtcars
mydata <- mtcars[, c("mpg", "drat")]
```

## Visualizing the data

```{r}
plot(mydata, xlim = c(5, 35), ylim = c(2, 5))
```

## Ellipse

```{r, echo = FALSE}
library(car)
dataEllipse(mydata$mpg,mydata$drat, levels = 0.95, xlim = c(5, 35), ylim = c(2, 5), xlab = "mpg", ylab = "drat")

```

*How would you draw two axis that capture most of the variation?*

## Considerations

There are few things to keep in mind before performing a PCA.

## Consideration #1

For practical reasons (i.e. to make computations easier), PCA should **always** be performed on **mean-centered** data (variables with mean = 0). 

This simply means that mean of the variable is subtracted from each observation.

$$
\bar{X}_j = \sum_{i = 1}^{n} X_{ij} = 0
$$

## Consideration #2

Additionally, variables should be **scaled** to **unit-variance** when $X_1, X_2, ... X_p$ are not of the same units.

Example:

>- $X_1$: Age of people $(1 \leq X_1 \leq 100)$
>- $X_2$: Salary income $(20 000 \$ \leq X_2 \leq 100 000 \$)$


## Original data

Should we scale?

```{r, echo = FALSE}
dataEllipse(mydata$mpg,mydata$drat, levels = 0.95, xlim = c(5, 35), ylim = c(2, 5), xlab = "mpg", ylab = "drat")
```

----

Since we do not have same unite, we first need to scale data to zero mean and unit-variance.

```{r ellipse, fig.width = 6, fig.height = 5, echo = -2:-4}
mydata_scaled <- scale(mydata, center = TRUE, scale = TRUE)

dataEllipse(mydata_scaled[, 1], 
            mydata_scaled[, 2], 
            levels = 0.95, 
            xlim = c(-2.5, 2.5), 
            ylim = c(-3, 3), 
            xlab = "mpg", 
            ylab = "drat",
            asp = 1)

```

```{r, echo = FALSE, results='hide'}
mydata_scaled <- data.frame(mydata_scaled)
names(mydata_scaled)
```

----

We can verify that the data has mean of 0.

```{r}
mean(mydata_scaled$mpg)   # Verify that mean is 0
mean(mydata_scaled$drat)  # Verify that mean is 0
```

----

We can verify that the data has variance of 1.

```{r}
var(mydata_scaled$mpg)    # Verify that variance is 1
var(mydata_scaled$drat)   # Verify that variance is 1
```

## Using covariance or correlation matrix

PCA are performed on either the **covariance** or **correlation** matrix of the data.

Which one to choose:

- **Covariance** if the data has the same unit.

- **Correlation** if the data has not the same unit (i.e. if you scaled data to unit-variance).

## Compute the correlation matrix

Given the data has not the same units we the first step is to calculate the correlation matrix on the variables of interest.

```{r}
my_cor <- cor(mydata_scaled) # Correlation matrix of scaled variables
my_cor # This is on this matrix we will perform the PCA
```

## SVD

The next step of a PCA is to perform a singular value decomposition (SVD) on the *correlation/covariance* matrix of the data.

>  SVD is a method for transforming correlated variables into a set of **uncorrelated ones** that better expose the various relationships among the original data items (Kirk Baker, 2013).

We will not go into the mathematic of it, but just keep in mind that this procedure will produce vectors that are **orthogonal** among each others.

## SVD

In R, SVD can be performed using the `eigen()` function.

```{r}
# Find the eigenvectors of the correlation matrix
my_eigen <- eigen(my_cor)

# Just renaming things so it is easier to understand
rownames(my_eigen$vectors) <- c("mpg", "drat")
colnames(my_eigen$vectors) <- c("PC1", "PC2")

my_eigen
```

## SVD

One important characteristic of eigenvectors/loadings is they are orthogonal (90 degree).

```{r}
cor(my_eigen$vectors) # Check for the correlation between PC
```

## Plotting the PC

At this point, the PCA is almost done. What we need to do is to plot the new axis (PCs/eigenvectors) on the scaled data.

```{r, echo = FALSE}
#my_eigen$vectors

loadings <- my_eigen$vectors

slope1 <- loadings[1, 1] / loadings[2, 1]
slope2 <- loadings[1, 2] / loadings[2, 2]

dataEllipse(mydata_scaled[, 1],
  mydata_scaled[, 2],
  levels = 0.95,
  xlim = c(-2.5, 2.5),
  ylim = c(-3, 3),
  xlab = "mpg",
  ylab = "drat",
  asp = 1
)

abline(0, slope1, col = "green", lwd = 3) ## Plot PC1
abline(0, slope2, col = "blue", lwd = 3) ## Plot PC2

legend("bottomright", c("PC1", "PC2"), col = c("green", "blue"), lty = 1, bty = "n", lwd = 2)
```

## Calculating scores

When you perform a PCA, **variables become the principal components**. 

What we want to do is to create a simple x, y plot with the newly created axes (loadings). **Scores** can be seen as the position of **scaled** data on the newly created axes (principal components).

This type of plot is called **biplot**.

```{r, eval = TRUE}
# Calculate the new positions of the observations on the PCs
scores <- as.matrix(mydata_scaled) %*% my_eigen$vectors
```

## Calculating scores

These values are called the **scores**. They are the **$x$** and **$y$** positions of the observations on the newly created axes (PCs).

<small>
```{r, echo = FALSE, results="asis"}
print(xtable::xtable(head(scores, 10)), type = "html", NA.string = "NA")
```
</small>

## Biplot

```{r, echo = FALSE, fig.width= 10}
par(mfrow = c(1, 2))
dataEllipse(mydata_scaled[, 1], 
            mydata_scaled[, 2], 
            levels = 0.95, 
            xlim = c(-2.5, 2.5), 
            ylim = c(-3, 3), 
            xlab = "mpg", 
            ylab = "drat",
            asp = 1, 
            pch = 21, bg = "gray")

abline(0, slope1, col = "green", lwd = 3) ## Plot PC1
abline(0, slope2, col = "blue", lwd = 3) ## Plot PC2

legend(0, -2, c("PC1", "PC2"), 
       col = c("green", "blue"), lty = 1, bty = "n", lwd = 2)

dataEllipse(scores[, 1], 
            scores[, 2], 
            levels = 0.95, 
            xlim = c(-2.5, 2.5), 
            ylim = c(-3, 3), 
            xlab = "PC1", 
            ylab = "PC2",
            asp = 1,
            pch = 21, bg = "gray")

abline(h = 0, col = "green", lwd = 3) ## Plot PC1
abline(v = 0, col = "blue", lwd = 3) ## Plot PC2
```


## Explained variance

We know that by definition that the generated PC are capturing most of the variation in the data. 

But how much is *most of the variation*?

Remember that the SVD generated eigenvectors (PC) and eigenvalues.

```{r}
my_eigen
```

## Explained variance

**An interesting property of the eigenvalues is their sum is equal to the total variance of the scaled variables.**

```{r}
# Sum of the generated eigenvalues
sum(my_eigen$values)

# Total variance of the scaled data
var(mydata_scaled$mpg) + var(mydata_scaled$drat)
```

## Explained variance

Hence, it becomes easy to calculate the variance explained by each PC.

```{r}
my_eigen$values

explained_variance <- my_eigen$values / sum(my_eigen$values)
explained_variance
```

We see that PC1 accounts for `r round(explained_variance[1]*100, digits = 2)`% of the total variance where as PC2 explains `r round(explained_variance[2]*100, digits = 2)`% of the total variance.

## Biplot

```{r, echo = FALSE}
dataEllipse(scores[, 1], 
            scores[, 2], 
            levels = 0.95, 
            xlim = c(-2.5, 2.5), 
            ylim = c(-3, 3), 
            xlab = "PC1 (84.06%)", 
            ylab = "PC2 (15.94%)",
            asp = 1,
            pch = 21, bg = "gray")

abline(h = 0, col = "green", lwd = 3) ## Plot PC1
abline(v = 0, col = "blue", lwd = 3) ## Plot PC2
```

## PCA in R

## PCA in R
<!-- http://www.davidzeleny.net/anadat-r/doku.php/en:pca -->

There are many packages to perform PCA in R. 

They are probably all good, but I recommend to use the `rda()` function from the `vegan` package.

The package is not included in base R, so you have to install it using the following code:

```{r, eval = FALSE}
install.packages("vegan")
library(vegan)
```

## PCA in R

We will use the same data to see if we can obtain the same results with `vegan`.

```{r}
head(mydata)
```

## PCA in R

A PCA is performed using the `rda()` function. It is a bit confusing since it is the same function for performing redundancy analysis.

```{r}
library(vegan)
my_pca <- rda(mydata, scale = TRUE) # Do not forget to scale!
my_pca
```

## PCA in R

Lets compare preliminary results from vegan with those we manually calculated earlier.

```{r}
my_pca # Vegan results

my_eigen$values # Manually calculated results
```

## Biplot in R

Biplot of a PCA can be easily plotted using the `biplot()` function.

## Biplot in R

```{r}
biplot(my_pca)
```

## A more complicated example

Lets do a PCA with more more variable (i.e. more dimensions).

```{r}
mydata <- head(mtcars[, 1:7])
mydata
```

## The PCA

```{r}
my_pca <- rda(mtcars[, 1:7], scale = TRUE) # Do not forget to scale!
my_pca # 7 PC generated
```

## The PCA

```{r}
summary(my_pca)
```

## Correlation between PC

As already stated, all principal component are orthogonal to each other.

```{r, echo = FALSE}
knitr::kable(cor(my_pca$CA$u), 
             caption = "Correlation coefficients between PC.")
```

## Selecting PCA axes

>- As we just saw, seven PC were generated. 
>- Since we are limited to plot into only 2D, we have to decide which PC to keep for the visualization. 
>- There are many ways to determine with PC are important which are review in @Peres-Neto2005.
>- Today we will focus on the **Kaiser-Guttman criterion**.

## Kaiser-Guttman criteria

The Kaiser-Guttman criteria simply says that PC with eigenvalues higher than the average of all eigenvalues should be kept.

We can use the `screeplot()` function to have a graphical overview of all eigenvalues.

----

```{r, fig.width = 5, fig.height = 4}
# plot the screeplot
screeplot(my_pca)

# add a line representing mean value of all eigenvalues
abline(h = mean(my_pca$CA$eig), col = "red") 
```

Based on this graph, it seems that the first two PC are important.

## Biplot

```{r}
biplot(my_pca, choices = c(1, 2)) # Selecting axes 1 and 2
```

## Selecting different axes

```{r}
biplot(my_pca, choices = c(1, 3)) # Selecting axes 1 and 3
```

## Interpreting a PCA

#### Angle between variables

Look at the angle between `qsec` and `wt` (~90 degrees).

```{r, echo = FALSE}
languageR::pairscor.fnc(mtcars[, 1:7])
```

## Your turn!

Using your data (or a subset of your data) and perform a PCA. What does this tell you about your data?

## Useful resources

<small>

https://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf

http://www.davidzeleny.net/anadat-r/doku.php/en:pca

</small>

## References  {.refsection}
